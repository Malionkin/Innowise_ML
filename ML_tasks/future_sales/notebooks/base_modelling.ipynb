{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting lightgbm\n",
      "  Downloading lightgbm-3.3.3-py3-none-win_amd64.whl (1.0 MB)\n",
      "Requirement already satisfied: numpy in d:\\study\\anaconda\\lib\\site-packages (from lightgbm) (1.23.5)\n",
      "Requirement already satisfied: wheel in d:\\study\\anaconda\\lib\\site-packages (from lightgbm) (0.35.1)\n",
      "Requirement already satisfied: scipy in d:\\study\\anaconda\\lib\\site-packages (from lightgbm) (1.5.2)\n",
      "Requirement already satisfied: scikit-learn!=0.22.0 in d:\\study\\anaconda\\lib\\site-packages (from lightgbm) (0.23.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in d:\\study\\anaconda\\lib\\site-packages (from scikit-learn!=0.22.0->lightgbm) (2.1.0)\n",
      "Requirement already satisfied: joblib>=0.11 in d:\\study\\anaconda\\lib\\site-packages (from scikit-learn!=0.22.0->lightgbm) (0.17.0)\n",
      "Installing collected packages: lightgbm\n",
      "Successfully installed lightgbm-3.3.3\n"
     ]
    }
   ],
   "source": [
    "!pip install lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "wM3_cndgwYOr"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "import lightgbm as lgb\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "gQG7yFRRSE6E"
   },
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hXsgsTHhWarj"
   },
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NpfpeaQnrY1Q",
    "outputId": "f9795677-e7cb-4f12-d737-f9a5a80e6fa7"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "./competitive-data-science-predict-future-sales/transformed_data/work_df_after_feature_engineering__train.parquet",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-b31d8d8936cc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mtest_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'./competitive-data-science-predict-future-sales/transformed_data/work_df_after_feature_engineering__test.parquet'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mdata_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'./competitive-data-science-predict-future-sales/transformed_data/work_df_after_feature_engineering__train.parquet'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_parquet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub_cat\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m25\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshop_id\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m9\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Study\\Anaconda\\lib\\site-packages\\pandas\\io\\parquet.py\u001b[0m in \u001b[0;36mread_parquet\u001b[1;34m(path, engine, columns, **kwargs)\u001b[0m\n\u001b[0;32m    315\u001b[0m     \"\"\"\n\u001b[0;32m    316\u001b[0m     \u001b[0mimpl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 317\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mimpl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\Study\\Anaconda\\lib\\site-packages\\pandas\\io\\parquet.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, path, columns, **kwargs)\u001b[0m\n\u001b[0;32m    139\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"use_pandas_metadata\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 141\u001b[1;33m         result = self.api.parquet.read_table(\n\u001b[0m\u001b[0;32m    142\u001b[0m             \u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilesystem\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m         ).to_pandas()\n",
      "\u001b[1;32mD:\\Study\\Anaconda\\lib\\site-packages\\pyarrow\\parquet\\core.py\u001b[0m in \u001b[0;36mread_table\u001b[1;34m(source, columns, use_threads, metadata, schema, use_pandas_metadata, memory_map, read_dictionary, filesystem, filters, buffer_size, partitioning, use_legacy_dataset, ignore_prefixes, pre_buffer, coerce_int96_timestamp_unit, decryption_properties, thrift_string_size_limit, thrift_container_size_limit)\u001b[0m\n\u001b[0;32m   2822\u001b[0m             )\n\u001b[0;32m   2823\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2824\u001b[1;33m             dataset = _ParquetDatasetV2(\n\u001b[0m\u001b[0;32m   2825\u001b[0m                 \u001b[0msource\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2826\u001b[0m                 \u001b[0mschema\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mschema\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Study\\Anaconda\\lib\\site-packages\\pyarrow\\parquet\\core.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, path_or_paths, filesystem, filters, partitioning, read_dictionary, buffer_size, memory_map, ignore_prefixes, pre_buffer, coerce_int96_timestamp_unit, schema, decryption_properties, thrift_string_size_limit, thrift_container_size_limit, **kwargs)\u001b[0m\n\u001b[0;32m   2421\u001b[0m                 infer_dictionary=True)\n\u001b[0;32m   2422\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2423\u001b[1;33m         self._dataset = ds.dataset(path_or_paths, filesystem=filesystem,\n\u001b[0m\u001b[0;32m   2424\u001b[0m                                    \u001b[0mschema\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mschema\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparquet_format\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2425\u001b[0m                                    \u001b[0mpartitioning\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpartitioning\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Study\\Anaconda\\lib\\site-packages\\pyarrow\\dataset.py\u001b[0m in \u001b[0;36mdataset\u001b[1;34m(source, schema, format, filesystem, partitioning, partition_base_dir, exclude_invalid_files, ignore_prefixes)\u001b[0m\n\u001b[0;32m    750\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    751\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0m_is_path_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msource\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 752\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_filesystem_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msource\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    753\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msource\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtuple\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    754\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_is_path_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0melem\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0melem\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msource\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Study\\Anaconda\\lib\\site-packages\\pyarrow\\dataset.py\u001b[0m in \u001b[0;36m_filesystem_dataset\u001b[1;34m(source, schema, filesystem, partitioning, format, partition_base_dir, exclude_invalid_files, selector_ignore_prefixes)\u001b[0m\n\u001b[0;32m    442\u001b[0m         \u001b[0mfs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpaths_or_selector\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_ensure_multiple_sources\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msource\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilesystem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    443\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 444\u001b[1;33m         \u001b[0mfs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpaths_or_selector\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_ensure_single_source\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msource\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilesystem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    445\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    446\u001b[0m     options = FileSystemFactoryOptions(\n",
      "\u001b[1;32mD:\\Study\\Anaconda\\lib\\site-packages\\pyarrow\\dataset.py\u001b[0m in \u001b[0;36m_ensure_single_source\u001b[1;34m(path, filesystem)\u001b[0m\n\u001b[0;32m    418\u001b[0m         \u001b[0mpaths_or_selector\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    419\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 420\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    421\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    422\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mfilesystem\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpaths_or_selector\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: ./competitive-data-science-predict-future-sales/transformed_data/work_df_after_feature_engineering__train.parquet"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "test_path = './competitive-data-science-predict-future-sales/transformed_data/work_df_after_feature_engineering__test.parquet'\n",
    "data_path = './competitive-data-science-predict-future-sales/transformed_data/work_df_after_feature_engineering__train.parquet'\n",
    "data = pd.read_parquet(data_path) \n",
    "data = data[data.sub_cat != 25]\n",
    "data = data[data.shop_id != 9]\n",
    "print(len(data))\n",
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gb6vQb5_wurD"
   },
   "outputs": [],
   "source": [
    "sample = pd.read_csv('sample_submission.csv')\n",
    "test = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pOfAvKs4Wjgd"
   },
   "source": [
    "# Constants and cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zMrqbgVZgScP"
   },
   "outputs": [],
   "source": [
    "start_month, end_month = (24, 32)\n",
    "predict_month = 28\n",
    "train_size = 6\n",
    "months = list(range(start_month,end_month))\n",
    "cat_cols = [1,2,3]\n",
    "\n",
    "num_cols = [#num cols]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0Z0k7EGcIVAG"
   },
   "source": [
    "# Functions (timesplit, cross-validation, grid search, data preparation, submisson )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LB9YqcutlD7W"
   },
   "outputs": [],
   "source": [
    "def root_mean_squared_error(true, pred):\n",
    "    return mean_squared_error(true, pred, squared=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HBdCmg47sdzb"
   },
   "source": [
    "### Cross - validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I3M3s_QGJSvq"
   },
   "outputs": [],
   "source": [
    "from ctypes import ArgumentError\n",
    "class TimeSeriesSplitter:\n",
    "    def __init__ (self, X, dates, date_index, min_train_size, rolling = True):\n",
    "        self.X = X\n",
    "        self.dates = dates\n",
    "        self.date_index = date_index\n",
    "        self.min_train_size = min_train_size\n",
    "        self.rolling = rolling\n",
    "\n",
    "    def split(self):\n",
    "        if self.min_train_size < 1:\n",
    "            raise ArgumentError()\n",
    "        prev = 0\n",
    "        iters = ((len(self.dates) - self.min_train_size - 1)) + 1\n",
    "        for i in range(iters):\n",
    "            start = prev if self.rolling else 0\n",
    "            end = prev + self.min_train_size\n",
    "            months = self.dates[start : end]\n",
    "            mask_train = np.isin(self.X[:, self.date_index], months)\n",
    "            mask_test = self.X[:, self.date_index] == self.dates[end]\n",
    "            X_train = np.where(mask_train)[0]\n",
    "            X_test = np.where(mask_test)[0]\n",
    "            prev += 1\n",
    "            yield X_train, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YgGSxkYCj07M"
   },
   "outputs": [],
   "source": [
    "def cross_val_score(X, y, model, cat_fet, months, train_size,  metric=root_mean_squared_error, plot = False, transformer = None, trial = None):\n",
    "    cv = TimeSeriesSplitter(X,months, 0, min_train_size=train_size).split()\n",
    "    errors = []\n",
    "    it = 0\n",
    "    for X_train_ind, X_test_ind in cv:\n",
    "        X_train, y_train, X_test, y_test = X[X_train_ind], y[X_train_ind], X[X_test_ind], y[X_test_ind]\n",
    "        X_train_ind, X_test_ind = 0, 0\n",
    "        if transformer:\n",
    "            X_train = transformer.fit_transform(X_train)\n",
    "            X_test = transformer.transform(X_test)\n",
    "        model.fit(X_train, y_train)\n",
    "        pr = model.predict(X_test)\n",
    "        pr = [0 if x < 0 else x for x in pr ]\n",
    "        errors.append(metric(y_test, pr))\n",
    "        it+=1\n",
    "    if plot:\n",
    "        plt.title('Cross-validation errors')\n",
    "        plt.plot(list(range(it)), errors)\n",
    "        plt.ylabel('RMSE')\n",
    "        plt.xlabel('Folds')\n",
    "        plt.show()\n",
    "    return errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m6tPM8Rmlx9r"
   },
   "outputs": [],
   "source": [
    "def validate(experiment, X, y, cat_cols, months, loss, train_size):\n",
    "        mlflow.set_experiment(experiment)\n",
    "        errors = train_mlflow(model, X, y, cat_cols, months, train_size, transformer)\n",
    "        print('ERRORS:  ', errors)\n",
    "        print('STD       MEAN       MEDIAN')\n",
    "        print(np.std(errors), np.mean(errors), np.median(errors))\n",
    "        lgb.plot_metric(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2oE0KrbfsjcM"
   },
   "source": [
    "### Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PgR7DyIyXRJ8"
   },
   "outputs": [],
   "source": [
    "def prepare_num_data(data, min_block, max_block, cols, transformer=None):\n",
    "    data = data[(data.date_block_num >= min_block) & (data.date_block_num <= max_block)]\n",
    "    y = data['item_cnt_month'].to_numpy()\n",
    "    y[y < 0] = 0\n",
    "    data = data[cols]\n",
    "    X = data.to_numpy()\n",
    "    if transformer:\n",
    "        transformer.fit(data)\n",
    "        data = transformer.transform(data)\n",
    "    return (X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pWedIklBJChY"
   },
   "outputs": [],
   "source": [
    "def make_sabmission(model, num_cols, subm_name, transformer = None):\n",
    "    data = pd.read_parquet(test_path)\n",
    "    test_data = pd.merge(test, data, on = ['shop_id', 'item_id'], how = 'left')\n",
    "    test_data = test_data[num_cols]\n",
    "    test_data = test_data.to_numpy()\n",
    "    if transformer:\n",
    "        test_data = transformer.transform(test_data)\n",
    "    prediction = model.predict(test_data)\n",
    "   \n",
    "    sample['item_cnt_month'] = prediction\n",
    "    sample.item_cnt_month = [x if x > 0 else 0 for x in sample.item_cnt_month]\n",
    "    sample.to_csv(subm_name, sep = ',', index = False)\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "siwiPbSUkH8q"
   },
   "outputs": [],
   "source": [
    "class CombinationModel():\n",
    "    def __init__(self, internet_model, cifr_model, model, internet_cols, city_index):\n",
    "         self.i_model = internet_model\n",
    "         self.c_model = cifr_model\n",
    "         self.model = model\n",
    "         self.i_cols = internet_cols\n",
    "         self.city_index = city_index\n",
    "         self.internet_cols = internet_cols\n",
    "    \n",
    "\n",
    "    def fit(self, X, y, categorical_feature ):\n",
    "        mask = X[:, self.city_index]==7\n",
    "        self.i_model.fit(X[mask], y[mask],categorical_feature=categorical_feature)\n",
    "        mask = X[:, self.city_index]==27\n",
    "        self.c_model.fit(X[mask], y[mask],categorical_feature=categorical_feature)\n",
    "        mask = np.logical_not(np.isin(X[:, self.city_index], self.internet_cols))\n",
    "        self.model.fit(X[mask],y[mask],categorical_feature=categorical_feature)\n",
    "\n",
    "    def predict(self, X):\n",
    "        prediction = np.zeros(len(X))\n",
    "        mask = X[:, self.city_index] == 7\n",
    "        pred = self.i_model.predict(X[mask])\n",
    "        prediction[np.where(mask)[0]] = pred\n",
    "        mask = X[:, self.city_index] == 27\n",
    "        pred = self.c_model.predict(X[mask])\n",
    "        prediction[np.where(mask)[0]] = pred\n",
    "        mask = np.logical_not(np.isin(X[:, self.city_index], self.internet_cols))\n",
    "        pred = self.model.predict(X[mask])\n",
    "        prediction[np.where(mask)[0]] = pred\n",
    "        mask = 0\n",
    "        return prediction\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L7HwDzPuswLm"
   },
   "source": [
    "### Param tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V631Ti65JmFb"
   },
   "outputs": [],
   "source": [
    "def suggest_params(trial):\n",
    "    param = {\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 50, 300, step=10),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.1, 0.4),\n",
    "        \"num_leaves\": trial.suggest_int(\"num_leaves\", 15, 200),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 4, 10),\n",
    "        \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 50, 370, step=20),\n",
    "        \"reg_alpha\": trial.suggest_float(\n",
    "            \"reg_alpha\", 1, 5, step=0.5),\n",
    "        \"reg_lambda\": trial.suggest_float(\n",
    "            \"reg_lambda\", 1, 5, step=0.5),\n",
    "        \"bagging_fraction\": trial.suggest_float(\n",
    "            \"bagging_fraction\", 0.2, 0.95, step=0.15),\n",
    "        \"feature_fraction\": trial.suggest_float(\n",
    "            \"feature_fraction\", 0.2, 0.95, step=0.1\n",
    "        )}\n",
    "    return param"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kez7dSLji-9U"
   },
   "source": [
    "# Train cycle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ry7A9Ug5jD5H"
   },
   "outputs": [],
   "source": [
    "X, y = prepare_num_data(data, start_month, end_month, num_cols, transformer=None)\n",
    "X_test, y_test  = prepare_num_data(data, start_month, end_month, num_cols, transformer=None)\n",
    "data = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-XKmXDkUjM4R"
   },
   "outputs": [],
   "source": [
    "model = LGBMRegressor(bagging_fraction=0.8, feature_fraction=0.7, learning_rate=0.17, \n",
    "                      min_child_samples=100, n_estimators=180,\n",
    "                      num_leaves=200, objective='huber', reg_alpha=3.5, reg_lambda=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aTT8k8f3Iu2p"
   },
   "outputs": [],
   "source": [
    "cross_val_score(X, y, comb_model, None, months, train_size, plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2t-IcsOwmHsz"
   },
   "outputs": [],
   "source": [
    "params = (X, y, cat_cols, months, 'regression_l1', 4)\n",
    "best_params = tune_params(15, *params)\n",
    "model.set_params(**best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0xZjAhc0jMBj"
   },
   "outputs": [],
   "source": [
    "validate('lgbm', *params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "35vza4QbpQ29"
   },
   "outputs": [],
   "source": [
    "print(len(X_test))\n",
    "model.fit(X_test, y_test, categorical_feature=cat_cols)\n",
    "make_sabmission(model, num_cols, 'comb_lgbm.csv', transformer=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "moqKF2eNH2lf"
   },
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Phr9DfDqoKQ6"
   },
   "outputs": [],
   "source": [
    "def category_errors(cats, cat_col, X, y, model, months, train_size):\n",
    "    cv = TimeSeriesSplitter(X,months, 0, min_train_size=train_size).split()\n",
    "    cats_errors = [{},{},{}]\n",
    "    for i, col in enumerate(cat_col):\n",
    "            for cat in cats[i]:\n",
    "                cats_errors[i][cat] = []\n",
    "    for X_train_ind, X_test_ind in cv:\n",
    "        X_train, y_train, X_test, y_test = X[X_train_ind], y[X_train_ind], X[X_test_ind], y[X_test_ind]\n",
    "        X_train_ind, X_test_ind = 0, 0\n",
    "        print(X_test[0,0])\n",
    "        model.fit(X_train, y_train, categorical_feature=cat_cols)\n",
    "        pr = model.predict(X_test)\n",
    "        pr = np.array([0 if x < 0 else x for x in pr ])\n",
    "        for i, col in enumerate(cat_col):\n",
    "            for cat in cats[i]:\n",
    "                mask = X_test[:,cat_col[i]] == cat\n",
    "                y_test_cat = y_test[mask]\n",
    "                pred_cat = pr[mask]\n",
    "                cats_errors[i][cat].append(root_mean_squared_error(y_test_cat, pred_cat))\n",
    "    return cats_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XKl0jguTQLnI"
   },
   "outputs": [],
   "source": [
    "class CategoryDecoder():\n",
    "    def __init__(self,cats_path, sub_cats_path, cities_path):\n",
    "        self.cats = pd.read_csv(cats_path, index_col='index')['0'].to_dict()\n",
    "        self.sub_cats = pd.read_csv(sub_cats_path, index_col='index')['0'].to_dict()\n",
    "        self.cities = pd.read_csv(cities_path,index_col='index')['0'].to_dict()\n",
    "    def _decode(self, col, index):\n",
    "        if col == 'city':\n",
    "            return self.cities[index]\n",
    "        if col == 'cat':\n",
    "            return self.cats[index]\n",
    "        if col == 'sub_cat':\n",
    "            return self.sub_cats[index]\n",
    "        else:\n",
    "             raise ArgumentException()\n",
    "    \n",
    "    def decode(self, d, col):\n",
    "        res = []\n",
    "        for x in d.keys():\n",
    "            res.append(self._decode(col, int(x)))\n",
    "        return res"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "O4I7fMdC2fWS",
    "t7vISCic_-Y7",
    "Itg-mkjtWX8W",
    "0Z0k7EGcIVAG",
    "HBdCmg47sdzb",
    "L7HwDzPuswLm",
    "T3160Fk3kp2i",
    "by-kMfpISD4J",
    "moqKF2eNH2lf",
    "3s-pM7gPH7pl",
    "SW-7d5zh_Zva",
    "a4qvB85H_d-Y",
    "8lqaNL5MF1Uw",
    "Av1t0MUwaG62",
    "Id1iXhIG1QcQ",
    "a4U3FRk-eH5V",
    "t9hSAoWGNz_3",
    "wbr2pxKQtoX-",
    "S6cKPTvkMese"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
